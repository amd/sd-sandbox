# Copyright (C) 2025 Advanced Micro Devices, Inc.  All rights reserved. Portions of this file consist of AI-generated content.

"""
Pipeline execution utilities.

Functions for building commands, executing pipelines, and managing execution providers.
"""

import os
import sys
import time
import json
import subprocess
import argparse
from pathlib import Path
from datetime import datetime
from collections import deque
from typing import Dict, List, Optional, Any, Callable, Tuple

# Import helper functions
from pipeline_helpers import (
    filter_extra_args, extract_arg_value, get_dimension_value, create_result_dict
)


# ============================================================================
# Command Building Functions
# ============================================================================

def build_command(script_path: Path, script_name: str, model_id: str, 
                  profiling_rounds: int, extra_args: List[str], image_only: bool = False) -> List[str]:
    """
    Construct the command line arguments for running a pipeline script.
    
    Creates model-specific output directories and handles script-specific argument
    filtering to prevent unsupported arguments from causing failures.
    
    Args:
        script_path (Path): Full path to the pipeline script to execute
        script_name (str): Name of the script file (for filtering logic)
        model_id (str): Hugging Face model identifier
        profiling_rounds (int): Number of profiling rounds to run
        extra_args (List[str]): Additional arguments from configuration
        image_only (bool): If True, run in batch mode without profiling (default: False)
        
    Returns:
        List[str]: Complete command list ready for subprocess execution
    """
    # Use relative path since we run from test directory
    cmd = [sys.executable, script_name]
    
    # Add arguments based on script type - let scripts use their default output locations
    # Note: --enable_profile triggers profiling mode, --no_excel disables Excel files
    # REMOVED --no_images from default: images now generated by default, use --no_images flag to disable
    if image_only:
        # Batch mode: no profiling, no warmup, fastest execution
        common_args = ["--verbose", "--no_excel"]
    else:
        # Profiling mode: includes warmup and profiling rounds
        common_args = ["--enable_profile", "--verbose", "--no_excel"]  # MODIFIED: Removed --no_images to allow image generation by default
    
    if script_name in ["run_sd15_controlnet.py"]:
        # SD15 controlnet script doesn't support --output_image
        # FIXED: Added missing --model_id and --profiling_rounds arguments
        filtered_extra_args = filter_extra_args(extra_args, ["--output_image"])
        cmd.extend(["--model_id", model_id] + common_args + 
                  ["--profiling_rounds", str(profiling_rounds)] + filtered_extra_args)
    elif script_name in ["run_sd30_controlnet.py"]:
        # SD30 controlnet script doesn't support --output_image
        filtered_extra_args = filter_extra_args(extra_args, ["--output_image"])
        cmd.extend(common_args + ["--profiling_rounds", str(profiling_rounds)] + filtered_extra_args)
    else:
        # Most other scripts don't support --output_image
        filtered_extra_args = filter_extra_args(extra_args, ["--output_image"])
        
        # Auto-add --dynamic_shape for SD3/SD3.5 scripts if not already present
        if script_name in ["run_sd3_dynamic.py", "run_sd3_controlnet_outpainting.py"]:
            if "--dynamic_shape" not in filtered_extra_args:
                filtered_extra_args.append("--dynamic_shape")
        
        cmd.extend(["--model_id", model_id] + common_args + 
                  ["--profiling_rounds", str(profiling_rounds)] + filtered_extra_args)
    
    return cmd


# ============================================================================
# Streaming Output Detection Function
# ============================================================================

def has_streaming_output(pipeline_configs: List[Dict[str, Any]], config_defaults: Optional[Dict[str, Any]]) -> bool:
    """
    Check if any pipeline in the configuration has streaming output enabled.
    
    Args:
        pipeline_configs (List[Dict[str, Any]]): List of pipeline configurations
        config_defaults (Optional[Dict[str, Any]]): Default configuration values
        
    Returns:
        bool: True if any pipeline has streaming output enabled
    """
    for config_item in pipeline_configs:
        # Determine streaming output setting using same logic as run_pipeline
        streaming_enabled = config_item.get('streaming_output')
        if streaming_enabled is None and config_defaults:
            streaming_enabled = config_defaults.get('streaming_output', True)
        elif streaming_enabled is None:
            streaming_enabled = True
        
        if streaming_enabled:
            return True
    return False


# ============================================================================
# Execution Provider Setup Functions
# ============================================================================

def setup_execution_provider(args: argparse.Namespace, config: Dict[str, Any]):
    """
    Configure execution providers based on command line arguments and config settings.
    
    Sets environment variables to control ONNX Runtime execution providers:
    - CPU-only execution (disables DML and other accelerators)
    - Provider selection based on configuration
    
    Args:
        args (argparse.Namespace): Parsed command-line arguments
        config (Dict[str, Any]): Configuration dictionary from YAML
    """
    # Get execution settings from config
    execution_config = config.get('defaults', {}).get('execution', {})
    force_cpu_config = execution_config.get('force_cpu', False)
    
    # Command line overrides config
    force_cpu = args.force_cpu or force_cpu_config
    
    if force_cpu:
        # Force CPU-only execution - disable all GPU and accelerator providers
        os.environ["ORT_DISABLE_GPU"] = "1"
        print("ðŸ–¥ï¸  CPU-only execution enabled:")
        print("   â€¢ GPU providers disabled (DML)")
        print("   â€¢ All accelerators will fall back to CPU")
    else:
        print("[AUTO] Automatic provider selection enabled")
        print("   â€¢ Will use best available: DML > CPU")
    
    # Show what execution providers will be attempted
    try:
        import onnxruntime as ort
        available_providers = ort.get_available_providers()
        print(f"   â€¢ Available providers: {', '.join(available_providers)}")
    except ImportError:
        print("   â€¢ ONNX Runtime not available for provider detection")


def setup_pipeline_execution_provider(config_item: Dict[str, Any], args: argparse.Namespace, config: Dict[str, Any]):
    """
    Configure execution providers for a specific pipeline.
    
    Pipeline-specific execution settings override global defaults and command line args.
    Priority: Pipeline config > Command line args > Global config defaults
    
    Args:
        config_item (Dict[str, Any]): Individual pipeline configuration
        args (argparse.Namespace): Parsed command-line arguments  
        config (Dict[str, Any]): Full configuration dictionary
    """
    # Get execution settings with priority order
    pipeline_execution = config_item.get('execution', {})
    global_execution = config.get('defaults', {}).get('execution', {})
    
    # Determine final settings (pipeline overrides command line overrides global)
    force_cpu = (
        pipeline_execution.get('force_cpu') or 
        args.force_cpu or 
        global_execution.get('force_cpu', False)
    )
    
    # Apply execution provider settings
    if force_cpu:
        os.environ["ORT_DISABLE_GPU"] = "1"
        
        execution_info = f"ðŸ–¥ï¸  {config_item['name']} - CPU-only execution"
        if pipeline_execution.get('force_cpu'):
            execution_info += " (pipeline config)"
        elif args.force_cpu:
            execution_info += " (command line)"
        else:
            execution_info += " (global config)"
            
        print(execution_info)
    else:
        print(f"[AUTO] {config_item['name']} - Automatic provider selection")


# ============================================================================
# Pipeline Execution Helper Functions
# ============================================================================

def setup_pipeline_args(config_item: Dict[str, Any], benchmark_mode: bool, 
                       config_defaults: Optional[Dict[str, Any]]) -> Tuple[int, List[str], int, str]:
    """
    Setup and prepare arguments for pipeline execution.
    
    Args:
        config_item: Pipeline configuration from YAML
        benchmark_mode: Whether to run in benchmark mode
        config_defaults: Default configuration values
        
    Returns:
        Tuple of (profiling_rounds, extra_args, num_inference_steps, model_path)
    """
    profiling_rounds = 10 if benchmark_mode else 1
    extra_args = config_item["extra_args"].copy()
    
    # Add global default arguments from config if specified
    if config_defaults and "default_args" in config_defaults:
        # Prepend default args so pipeline-specific args can override them
        extra_args = config_defaults["default_args"] + extra_args
    
    # Extract metadata for result tracking
    num_inference_steps = extract_arg_value(extra_args, ["--num_inference_steps", "-n"], 0, as_int=True)
    model_path = extract_arg_value(extra_args, ["--model_path"], None, as_int=False)
    
    return profiling_rounds, extra_args, num_inference_steps, model_path


def get_pipeline_resolution(extra_args: List[str], config_item: Dict[str, Any], 
                           config_defaults: Optional[Dict[str, Any]]) -> str:
    """
    Get the resolution string for this pipeline execution.
    
    Args:
        extra_args: Command line arguments (modified in place)
        config_item: Pipeline configuration
        config_defaults: Default configuration values
        
    Returns:
        Resolution string (e.g., "512x512")
    """
    width = get_dimension_value("width", extra_args, config_item, config_defaults)
    height = get_dimension_value("height", extra_args, config_item, config_defaults)
    return f"{width}x{height}"


def handle_sd3_controlnet_mode(script_name: str, prompt_file_path: Optional[str],
                               extra_args: List[str], sd3_controlnet_mode: str,
                               prompt_description: str) -> str:
    """
    Handle special SD3 ControlNet mode conversions.
    
    Args:
        script_name: Name of the pipeline script
        prompt_file_path: Path to prompt file if provided
        extra_args: Command line arguments (modified in place)
        sd3_controlnet_mode: Mode for SD3 ControlNet ("controlnet" or "text2img")
        prompt_description: Current prompt description
        
    Returns:
        Updated prompt description
    """
    if script_name != "run_sd30_controlnet.py" or not prompt_file_path or "--controlnet" not in extra_args:
        return prompt_description
    
    try:
        controlnet_idx = extra_args.index("--controlnet")
        if controlnet_idx + 1 >= len(extra_args):
            return prompt_description
            
        controlnet_type = extra_args[controlnet_idx + 1].lower()
        if controlnet_type == "none":
            return prompt_description
        
        if sd3_controlnet_mode == "text2img":
            # User prefers text-to-image mode over ControlNet mode
            print(f"  [Mode] Converting SD3 ControlNet {controlnet_type.title()} to text-to-image mode...")
            print(f"  [Info] Will use custom prompts without ControlNet guidance")
            
            # Change ControlNet mode to "None" to enable text-to-image mode
            extra_args[controlnet_idx + 1] = "None"
            
            # Update description to reflect the change
            try:
                with open(prompt_file_path, 'r', encoding='utf-8') as f:
                    prompts = json.load(f)
                return f"SD3 text-to-image mode with {len(prompts)} prompts (converted from {controlnet_type.title()} ControlNet)"
            except Exception as e:
                print(f"  [Warning] Could not read prompt file for SD3: {e}")
                return prompt_description
                
        elif sd3_controlnet_mode == "controlnet":
            # User wants to keep ControlNet mode - now supports custom prompts!
            print(f"  [OK] Using SD3 ControlNet {controlnet_type.title()} mode with custom prompts.")
            print(f"  [Info] Custom prompts from {prompt_file_path} will be used with ControlNet guidance.")
            
    except (ValueError, IndexError):
        # --controlnet not found or malformed, continue with normal processing
        pass
    
    return prompt_description


def setup_process_environment(source_path: Path) -> Dict[str, str]:
    """
    Setup environment variables for subprocess execution.
    
    Args:
        source_path: Path to project source directory
        
    Returns:
        Dictionary of environment variables
    """
    process_env = os.environ.copy()
    src_path = str((source_path / "src").resolve())
    utils_path = str((source_path / "src" / "utils").resolve())
    
    if "PYTHONPATH" in process_env:
        process_env["PYTHONPATH"] = f"{src_path};{utils_path};{process_env['PYTHONPATH']}"
    else:
        process_env["PYTHONPATH"] = f"{src_path};{utils_path}"
    
    # Ensure PYTHONUNBUFFERED is set for real-time output
    process_env["PYTHONUNBUFFERED"] = "1"
    
    return process_env


def stream_process_output(process: subprocess.Popen, streaming_enabled: bool, 
                         benchmark_mode: bool, profiling_rounds: int, prompt_count: int,
                         log_buffer: Optional[List[str]], 
                         pipeline_name: str, model_id: str) -> Tuple[deque, int]:
    """
    Stream and capture subprocess output with progress tracking.
    
    Args:
        process: The subprocess to stream from
        streaming_enabled: Whether to print output in real-time
        benchmark_mode: Whether running in benchmark mode
        profiling_rounds: Number of profiling rounds
        prompt_count: Number of prompts being processed
        log_buffer: Optional buffer for collecting log output
        pipeline_name: Name of the pipeline for logging
        model_id: Model ID for logging
        
    Returns:
        Tuple of (last_lines deque, rounds_completed count)
    """
    last_lines = deque(maxlen=9)
    
    # Stream output based on configuration
    if streaming_enabled:
        print("  [Output] Live Output:")
        # Add pipeline header to log buffer if streaming and log buffer exists
        if log_buffer is not None:
            log_buffer.append(f"\n--- Pipeline: {pipeline_name} | Model: {model_id} ---")
            log_buffer.append(f"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            log_buffer.append("[Output] Live Output:")
    else:
        # Show simple running indicator
        rounds_msg = f" ({profiling_rounds} rounds)" if benchmark_mode else ""
        prompts_msg = f" x {prompt_count} prompts" if prompt_count > 1 else ""
        print(f"  [Running]{rounds_msg}{prompts_msg}...")
    
    # Track progress for benchmark mode using round-complete markers
    rounds_completed = 0
    expected_rounds = profiling_rounds * prompt_count if benchmark_mode else 0
    
    while True:
        if process.stdout is None:
            break

        line = process.stdout.readline()
        if not line:
            break

        text = line.rstrip()
        
        # Add to log buffer if streaming is enabled and log buffer exists
        if streaming_enabled and log_buffer is not None:
            log_buffer.append(f"     {text}")
        
        # Parse round-complete markers emitted by pipeline triggers
        if text.startswith("__ROUND_COMPLETE__"):
            try:
                # Format: __ROUND_COMPLETE__ 3/10
                parts = text.split()
                if len(parts) >= 2 and "/" in parts[1]:
                    rounds_completed += 1
                    if benchmark_mode:
                        # Print progress immediately
                        progress_msg = f"  [Progress] {rounds_completed}/{expected_rounds} rounds completed"
                        if streaming_enabled:
                            print(progress_msg, flush=True)
                            # Add progress to log buffer
                            if log_buffer is not None:
                                log_buffer.append(progress_msg)
                        else:
                            print(progress_msg, end="\r", flush=True)
            except (ValueError, IndexError) as e:
                # Malformed round marker - log but don't crash
                if os.environ.get('DEBUG_PIPELINE'):
                    print(f"Warning: Could not parse round marker: {text}", file=sys.stderr)

        # Print the line immediately if streaming is enabled
        if streaming_enabled:
            print(f"     {text}")

        # Keep only last 9 lines for timing summary
        last_lines.append(text)
    
    return last_lines, rounds_completed


def finalize_execution_log(streaming_enabled: bool, log_buffer: Optional[List[str]],
                          success: bool, duration: float, stderr_output: str,
                          resolution: str, num_inference_steps: int):
    """
    Add completion information to log buffer.
    
    Args:
        streaming_enabled: Whether streaming was enabled
        log_buffer: Optional buffer for collecting log output
        success: Whether execution succeeded
        duration: Execution duration in seconds
        stderr_output: Error output if any
        resolution: Image resolution
        num_inference_steps: Number of inference steps
    """
    if streaming_enabled and log_buffer is not None:
        log_buffer.append(f"Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        log_buffer.append(f"Duration: {duration:.1f}s")
        log_buffer.append(f"Success: {'[OK] YES' if success else '[FAILED] NO'}")
        if not success:
            log_buffer.append(f"Error: {stderr_output}")
        log_buffer.append(f"Resolution: {resolution}")
        log_buffer.append(f"Inference Steps: {num_inference_steps}")
        log_buffer.append("-" * 60)


def handle_subprocess_timeout(process: subprocess.Popen, timeout: int, 
                              streaming_enabled: bool, log_buffer: Optional[List[str]],
                              script_name: str, model_id: str, pipeline_name: str,
                              start_time: float, resolution: str, 
                              num_inference_steps: int, model_path: Optional[str]) -> Dict[str, Any]:
    """
    Handle subprocess timeout and cleanup.
    
    Args:
        process: The subprocess that timed out
        timeout: Timeout value in seconds
        streaming_enabled: Whether streaming was enabled
        log_buffer: Optional buffer for collecting log output
        script_name: Name of the script
        model_id: Model ID
        pipeline_name: Pipeline name
        start_time: Execution start time
        resolution: Image resolution
        num_inference_steps: Number of inference steps
        model_path: Path to model files
        
    Returns:
        Result dictionary for timeout case
    """
    process.terminate()
    try:
        process.wait(timeout=5)  # Give it a few seconds to terminate
    except subprocess.TimeoutExpired:
        process.kill()
        process.wait()
    
    duration = time.time() - start_time
    
    # Add timeout info to log buffer if streaming is enabled and log buffer exists
    if streaming_enabled and log_buffer is not None:
        log_buffer.append(f"[FAILED] TIMEOUT after {timeout} seconds")
        log_buffer.append(f"Completed at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        log_buffer.append("-" * 60)
    
    return create_result_dict(script_name, model_id, pipeline_name, 
                            error=f"Timeout after {timeout} seconds", duration=timeout, 
                            resolution=resolution, num_inference_steps=num_inference_steps,
                            model_path=model_path)


# ============================================================================
# Main Pipeline Execution Function
# ============================================================================

def run_pipeline(config_item: Dict[str, Any], model_id: str, test_path: Path, source_path: Path,
                benchmark_mode: bool = False, timeout: int = 1200, custom_prompt: Optional[str] = None, 
                prompt_file_path: Optional[str] = None,
                sd3_controlnet_mode: str = "controlnet",
                config_defaults: Optional[Dict[str, Any]] = None,
                log_buffer: Optional[List[str]] = None,
                determine_prompt_source_func: Optional[Callable[[Dict[str, Any], Optional[str], Optional[str], Optional[Dict[str, Any]]], Tuple[List[str], str, int]]] = None,
                image_only: bool = False) -> Dict[str, Any]:
    """
    Execute a single pipeline configuration with metrics collection.
    
    Args:
        config_item: Pipeline configuration from YAML
        model_id: Friendly name for the model
        test_path: Path to test scripts
        source_path: Path to project source
        benchmark_mode: Whether to run in benchmark mode (10 profiling rounds)
        timeout: Max execution time in seconds
        custom_prompt: Custom prompt to override configuration
        prompt_file_path: Path to prompt file to use
        sd3_controlnet_mode: Mode for SD3 ControlNet ("controlnet" or "text2img")
        config_defaults: Optional default config for all pipelines
        log_buffer: Optional list to collect log output (only used if streaming_enabled=True)
        determine_prompt_source_func: Function to determine prompt source
        image_only: If True, run in batch mode without profiling
        
    Returns:
        Dictionary with execution results and metrics
    """
    pipeline_name = config_item["name"]
    script_name = config_item["script"]
    script_path = (test_path / script_name).resolve()
    
    # Determine streaming output setting
    streaming_enabled = config_item.get('streaming_output')
    if streaming_enabled is None and config_defaults:
        streaming_enabled = config_defaults.get('streaming_output', True)
    elif streaming_enabled is None:
        streaming_enabled = True
    
    if not script_path.exists():
        return create_result_dict(script_name, model_id, pipeline_name, 
                                error=f"Script {script_path} not found",
                                model_path=None)
    
    # Setup arguments and extract metadata
    profiling_rounds, extra_args, num_inference_steps, model_path = setup_pipeline_args(
        config_item, benchmark_mode, config_defaults
    )
    
    # Get resolution for result tracking
    resolution = get_pipeline_resolution(extra_args, config_item, config_defaults)
    
    # Handle prompt arguments using helper function
    if determine_prompt_source_func is None:
        raise ValueError("determine_prompt_source_func must be provided")
    prompt_args, prompt_description, prompt_count = determine_prompt_source_func(
        config_item, custom_prompt, prompt_file_path, config_defaults
    )
    extra_args.extend(prompt_args)
    
    # Handle SD3 ControlNet mode conversion if applicable  
    prompt_description = handle_sd3_controlnet_mode(
        script_name, prompt_file_path, extra_args, sd3_controlnet_mode, prompt_description
    )
    
    # Build and run command
    cmd = build_command(script_path, script_name, model_id, profiling_rounds, extra_args, image_only)
    
    # Print mode information
    if image_only:
        print(f"  [IMAGE-ONLY] Running in IMAGE-ONLY mode (batch mode, no profiling, fastest execution)")
    
    # Print command for debugging
    if os.environ.get('DEBUG_PIPELINE'):
        print(f"  [Debug] Command: {' '.join(cmd)}")
        print(f"  [Debug] Working dir: {test_path}")
    
    # Execute the pipeline
    start_time = time.time()
    
    try:
        # Setup environment
        process_env = setup_process_environment(source_path)
        
        # Use Popen for real-time output streaming
        process = subprocess.Popen(
            cmd, cwd=str(test_path), 
            stdout=subprocess.PIPE, 
            stderr=subprocess.STDOUT, 
            text=True, 
            env=process_env,
            universal_newlines=True, 
            bufsize=1
        )
        
        # Stream output and track progress
        last_lines, rounds_completed = stream_process_output(
            process, streaming_enabled, benchmark_mode, profiling_rounds, 
            prompt_count, log_buffer, pipeline_name, model_id
        )
        
        # Wait for the process to finish with timeout
        try:
            return_code = process.wait(timeout=timeout)
        except subprocess.TimeoutExpired:
            return handle_subprocess_timeout(
                process, timeout, streaming_enabled, log_buffer,
                script_name, model_id, pipeline_name, start_time,
                resolution, num_inference_steps, model_path
            )
        
        duration = time.time() - start_time
        success = return_code == 0
        stderr_output = f"Process exited with code {return_code}" if not success else ""
        
        # Convert deque to list for timing summary
        timing_lines = list(last_lines)
        
        # Finalize log
        finalize_execution_log(streaming_enabled, log_buffer, success, duration,
                              stderr_output, resolution, num_inference_steps)
        
        return create_result_dict(script_name, model_id, pipeline_name, success, duration, 
                                stderr_output, timing_lines, resolution, num_inference_steps,
                                model_path)
                                
    except Exception as e:
        return create_result_dict(script_name, model_id, pipeline_name, error=str(e), 
                                resolution=resolution, num_inference_steps=num_inference_steps,
                                model_path=model_path)

